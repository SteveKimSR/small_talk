{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crawler\n",
    "import word_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [00:00<00:00, 178863.71it/s]\n"
     ]
    }
   ],
   "source": [
    "cc = crawler.crawler()\n",
    "acl = cc.acl2023_crawler()\n",
    "\n",
    "f = open('./acl_titles.txt', 'w')\n",
    "for i in acl:\n",
    "    f.write(f'{i}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in acl:\n",
    "    titles.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "# Function to extract words and count their frequency\n",
    "def extract_and_count_words(titles):\n",
    "    word_counter = Counter()\n",
    "    for title in titles:\n",
    "        # Split title into words based on spaces and punctuation, then convert to lowercase\n",
    "        words = re.findall(r'\\b\\w+\\b', title.lower())\n",
    "        word_counter.update(words)\n",
    "    return word_counter\n",
    "\n",
    "# 단어 추출 및 빈도수 기록\n",
    "word_counts = extract_and_count_words(titles)\n",
    "# 정렬\n",
    "sorted_word_counts = word_counts.most_common()\n",
    "# 출력\n",
    "for word, count in sorted_word_counts:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순 영단어에 해당하는 빈도수는 출력이 가능 그러나 현재 전체적인 논문의 트렌드나, 연구중인 분야를 파악하기에는 적합한 결과가 나오지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the elephant in the room: analyzing the presence of big tech in natural language processing research', 'how about kind of generating hedges using end-to-end neural models?', 'what are the desired characteristics of calibration sets? identifying correlates on long form scientific summarization', 'generating edu extracts for plan-guided summary re-ranking', 'the cringe loss: learning what language not to model', 'multimodal persona based generation of comic dialogs', 'script normalization for unconventional writing of under-resourced languages in bilingual communities', 'mpchat: towards multimodal persona-grounded conversation', 'on-the-fly cross-lingual masking for multilingual pre-training', 'early discovery of disappearing entities in microblogs', 'prompter: zero-shot adaptive prefixes for dialogue state tracking domain adaptation', 'rl4f: generating natural language feedback with reinforcement learning for repairing model outputs', 'lexsym: compositionality as lexical symmetry', 'a diverse set of freely available linguistic resources for turkish', 'query refinement prompts for closed-book long-form qa', 'exploiting biased models to de-bias text: a gender-fair rewriting model', 'how do in-context examples affect compositional generalization?', 'disorbert: a double domain adaptation model for detecting signs of mental disorders in social media', 'topic-guided sampling for data-efficient multi-domain stance detection', 'unbalanced optimal transport for unbalanced word alignment', 'the kitmus test: evaluating knowledge integration from multiple sources', 'direct fact retrieval from knowledge graphs without entity linking', 'wukong-reader: multi-modal pre-training for fine-grained visual document understanding', 'syntax and geometry of information', 'rethinking the role of scale for in-context learning: an interpretability-based case study at 66 billion scale', 'controlling learned effects to reduce spurious correlations in text classifiers', 'target-side augmentation for document-level machine translation', 'a synthetic data generation framework for grounded dialogues', 'casn:class-aware score network for textual adversarial detection', 'human inspired progressive alignment and comparative learning for grounded word acquisition', 'making more of little data: improving low-resource automatic speech recognition using data augmentation', 'neurostructural decoding: neural text generation with structural constraints', 'span-selective linear attention transformers for effective and robust schema-guided dialogue state tracking', 'elqa: a corpus of metalinguistic questions and answers about english', 'bygpt5: end-to-end style-conditioned poetry generation with token-free language models', 'i2d2: inductive knowledge distillation with neurologic and self-imitation', 'crosssum: beyond english-centric cross-lingual summarization for 1,500+ language pairs', 'simplicity bias in transformers and their ability to learn sparse boolean functions', 'diffusemp: a diffusion model-based framework with multi-grained control for empathetic response generation', 'prompting language models for linguistic structure', 'simsum: document-level text simplification via simultaneous summarization', 'wikihowqa: a comprehensive benchmark for multi-document non-factoid question answering', 'multilingual event extraction from historical newspaper adverts', 'searching for needles in a haystack: on the role of incidental bilingualism in palm’s translation capability', 'measuring progress in fine-grained vision-and-language understanding', 'convergence and diversity in the control hierarchy', 'peek across: improving multi-document modeling via cross-document question-answering', 'query enhanced knowledge-intensive conversation via unsupervised joint modeling', 'generating user-engaging news headlines', 'a systematic study of knowledge distillation for natural language generation with pseudo-target training', 'what is the best recipe for character-level encoder-only modelling?', 'pumer: pruning and merging tokens for efficient vision language models', 'bridging the domain gaps in context representations for $k$-nearest neighbor neural machine translation', 'from key points to key point hierarchy: structured and expressive opinion summarization', 'improving gradient trade-offs between tasks in multi-task text classification', 'zero-shot approach to overcome perturbation sensitivity of prompts', 'lexfiles and legallama: facilitating english multinational legal language model development', 'few-shot adaptation works with unpredictable data', 'composition-contrastive learning for sentence embeddings', 'multi-cls bert: an efficient alternative to traditional ensembling', 'data curation alone can stabilize in-context learning', 'characterizing and measuring linguistic dataset drift', 'learning symbolic rules over abstract meaning representations for textual reinforcement learning', 'ideology prediction from scarce and biased supervision: learn to disregard the \"what” and focus on the \"how”!', 'weakly supervised vision-and-language pre-training with relative representations', 'label-aware hyperbolic embeddings for fine-grained emotion classification', 'mclip: multilingual clip via cross-lingual transfer', 'rev: information-theoretic evaluation of free-text rationales', 'did the models understand documents? benchmarking models for language understanding in document-level relation extraction', 'nonlinear structural equation model guided gaussian mixture hierarchical topic modeling', 'say what you mean! large language models speak too positively about negative commonsense knowledge', 'learning in-context learning for named entity recognition', 'exploring how generative adversarial networks learn phonological representations', 'tablevlm: multi-modal pre-training for table structure recognition', 'cheer: centrality-aware high-order event reasoning network for document-level event causality identification', 'blaser: a text-free speech-to-speech translation evaluation metric', 'alleviating over-smoothing for unsupervised sentence representation', 'consistent prototype learning for few-shot continual relation extraction', 'improving the robustness of summarization systems with dual augmentation', 'dsee: dually sparsity-embedded efficient tuning of pre-trained language models', 'a close look into the calibration of pre-trained language models', 'dynamic transformers provide a false sense of efficiency', 'pmaes: prompt-mapping contrastive learning for cross-prompt automated essay scoring', 'exploring lottery prompts for pre-trained language models', 'unisumm and summzoo: unified model and diverse benchmark for few-shot summarization', 'revisiting cross-lingual summarization: a corpus-based study and a new benchmark with improved annotation', 'disco: distilling counterfactuals with large language models', 'from the one, judge of the whole: typed entailment graph construction with predicate generation', 'causal intervention and counterfactual reasoning for multi-modal fake news detection', 'multi-granularity temporal question answering over knowledge graphs', 'explainable recommendation with personalized review retrieval and aspect learning', 'mdace: mimic documents annotated with code evidence', 'marked personas: using natural language prompts to measure stereotypes in language models', 'opensr: open-modality speech recognition via maintaining multi-modality alignment', 'dissecting transformer length extrapolation via the lens of receptive field analysis', 'can large language models be an alternative to human evaluations?', 'celda: leveraging black-box language model as enhanced classifier without labels', 'advancing multi-criteria chinese word segmentation through criterion classification and denoising', 'a method for studying semantic construal in grammatical constructions with interpretable contextual embedding spaces', 'increasing diversity while maintaining accuracy: text data generation with large language models and human interventions', 'rule by example: harnessing logical rules for explainable hate speech detection', 'a dynamic programming algorithm for span-based nested named-entity recognition in o(n^2)', 'laziness is a virtue when it comes to compositionality in neural semantic parsing', 'decoder tuning: efficient language understanding as decoding', 'adaptive and personalized exercise generation for online language learning', 'what does the failure to reason with \"respectively’’ in zero/few-shot settings tell us about language models?', 'free lunch for efficient textual commonsense integration in language models', 'from ultra-fine to fine: fine-tuning ultra-fine entity typing models to fine-grained', 'long-tailed question answering in an open world', 'detecting and mitigating hallucinations in machine translation: model internal workings alone do well, sentence similarity even better', 'analyzing transformers in embedding space', 'improving pretraining techniques for code-switched nlp', 'dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies', 'subset retrieval nearest neighbor machine translation', 'speech: structured prediction with energy-based event-centric hyperspheres', 'counterfactual active learning for out-of-distribution generalization', 'knowledge-enhanced mixed-initiative dialogue system for emotional support conversations', 'product question answering in e-commerce: a survey', 'towards faithful dialogues via focus learning', 'bidirectional generative framework for cross-domain aspect-based sentiment analysis', 'mixture-of-domain-adapters: decoupling and injecting domain knowledge to pre-trained language models’ memories', 'is gpt-3 a good data annotator?', 'masakhapos: part-of-speech tagging for typologically diverse african languages', 'unsupervised open-domain keyphrase generation', 'modeling what-to-ask and how-to-ask for answer-unaware conversational question generation', 'towards leaving no indic language behind: building monolingual corpora, benchmark and models for indic languages', 'cold fusion: collaborative descent for distributed multitask finetuning', 'generalizing backpropagation for gradient-based interpretability', 'a measure-theoretic characterization of tight language models', 'towards stable natural language understanding via information entropy guided debiasing', 'storywars: a dataset and instruction tuning baselines for collaborative story understanding and generation', 'measuring the instability of fine-tuning', 'to adapt or to annotate: challenges and interventions for domain adaptation in open-domain question answering', 'mad-tsc: a multilingual aligned news dataset for target-dependent sentiment classification', 'speechmatrix: a large-scale mined corpus of multilingual speech-to-speech translations', 'automatic annotation of direct speech in written french narratives', 'nlpeer: a unified resource for the computational study of peer review', 'how do humans perceive adversarial text? a reality check on the validity and naturalness of word-based adversarial attacks', 'hucurl: human-induced curriculum discovery', 'injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue', 'storyarg: a corpus of narratives and personal experiences in argumentative texts', 'manner: a variational memory-augmented model for cross domain few-shot named entity recognition', 'understanding and bridging the modality gap for speech translation', 'back translation for speech-to-text translation without transcripts', 'towards domain-agnostic and domain-adaptive dementia detection from spoken language', 'cross-lingual science journalism: select, simplify and rewrite summaries for non-expert readers', 'scene graph as pivoting: inference-time image-free unsupervised multimodal machine translation with visual scene hallucination', 'mitigating label biases for in-context learning', 'enhancing grammatical error correction systems with explanations', 'winoqueer: a community-in-the-loop benchmark for anti-lgbtq+ bias in large language models', 'joint constrained learning with boundary-adjusting for emotion-cause pair extraction', 'mmdialog: a large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation', 'kalm: knowledge-aware integration of local, document, and global contexts for long document understanding', 'from pretraining data to language models to downstream tasks: tracking the trails of political biases leading to unfair nlp models', 'generic temporal reasoning with differential analysis and explanation', 'schema-guided user satisfaction modeling for task-oriented dialogues', 'dunst: dual noisy self training for semi-supervised controllable text generation', 'when does translation require context? a data-driven, multilingual exploration', 'explaining how transformers use context to build predictions', 'don’t forget your abc’s: evaluating the state-of-the-art in chat-oriented dialogue systems', 'massive: a 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages', 'fairprism: evaluating fairness-related harms in text generation', 'matching pairs: attributing fine-tuned models to their pre-trained large language models', 'epic: multi-perspective annotation of a corpus of irony', 'conflicts, villains, resolutions: towards models of narrative media framing', 'on the compositional generalization in versatile open-domain dialogue', 'tackling ambiguity with images: improved multimodal machine translation and contrastive evaluation', 'learning answer generation using supervision from automatic question answering evaluators', 'question-answering in a low-resourced language: benchmark dataset and models for tigrinya', 'annotating mentions alone enables efficient domain adaptation for coreference resolution', 'livechat: a large-scale personalized dialogue dataset automatically constructed from live streaming', 'rarr: researching and revising what language models say, using language models', 'precise zero-shot dense retrieval without relevance labels', 'reference matters: benchmarking factual error correction for dialogue summarization with fine-grained evaluation framework', 'dialogue summarization with static-dynamic structure fusion graph', 'peacok: persona commonsense knowledge for consistent and engaging narratives', 'dsrm: boost textual adversarial training with distribution shift risk minimization', 'small pre-trained language models can be fine-tuned as large models via over-parameterization', 'entailment as robust self-learner', 'compounding geometric operations for knowledge graph completion', 'the benefits of bad advice: autocontrastive decoding across model layers', 'accent: an automatic event commonsense evaluation metric for open-domain dialogue systems', 'aclm: a selective-denoising based generative data augmentation approach for low-resource complex ner', 'multitask pretraining with structured knowledge for text-to-sql generation', 'interpretable word sense representations via definition generation: the case of semantic change analysis', 'model-generated pretraining signals improves zero-shot generalization of text-to-text transformers', 'mvp: multi-view prompting improves aspect sentiment tuple prediction', 'factual or contextual? disentangling error types in entity description generation', 'massively multilingual lexical specialization of multilingual transformers', 'rogue scores', 'gift: graph-induced fine-tuning for multi-party conversation understanding', 'a gradient control method for backdoor attacks on parameter-efficient tuning', 'don’t generate, discriminate: a proposal for grounding language models to real-world environments', 'do language models have coherent mental models of everyday things?', 'pre-training to learn in context', 'controllable text generation via probability density estimation in the latent space', 'on the evaluation of neural selective prediction methods for natural language processing', 'optimal transport for unsupervised hallucination detection in neural machine translation', 'hifi: high-information attention heads hold for parameter-efficient model adaptation', 'visually-augmented pretrained language models for nlp tasks without images', 'decoding symbolism in language models', 'dual cache for long document neural coreference resolution', 'learning optimal policy for simultaneous machine translation via binary search', 'counterfactual multihop qa: a cause-effect approach for reducing disconnected reasoning', 'analyzing and reducing the performance gap in cross-lingual transfer with fine-tuning slow and fast', 'bi-phone: modeling inter language phonetic influences in text', 'don’t retrain, just rewrite: countering adversarial perturbations by rewriting text', 'counterspeeches up my sleeve! intent distribution learning and persistent fusion for intent-conditioned counterspeech generation', 'discomat: distantly supervised composition extraction from tables in materials science articles', 'linguistic representations for fewer-shot relation extraction across domains', 'hybrid knowledge transfer for improved cross-lingual event detection via hierarchical sample selection', 'improving the detection of multilingual online attacks with rich social media data from singapore', 'text style transfer with contrastive transfer pattern mining', 'ssd-lm: semi-autoregressive simplex-based diffusion language model for text generation and modular control', 'understanding in-context learning via supportive pretraining data', 'bgglue: a bulgarian general language understanding evaluation benchmark', 'neural unsupervised reconstruction of protolanguage word forms', 'z-code++: a pre-trained language model optimized for abstractive summarization', 'hauser: towards holistic and automatic evaluation of simile generation', 'pad-net: an efficient framework for dynamic networks', 'on the blind spots of model-based evaluation metrics for text generation', 'hermes: interactive spreadsheet formula prediction via hierarchical formulet expansion', 'exploring the capacity of pretrained language models for reasoning about actions and change', 'revisiting event argument extraction: can eae models learn better when being aware of event co-occurrences?', 'targeted data generation: finding and fixing model weaknesses', 'diffusionbert: improving generative masked language models with diffusion models', 'umrspell: unifying the detection and correction parts of pre-trained models towards chinese missing, redundant, and spelling correction', 'language of bargaining', 'damp: doubly aligned multilingual parser for task-oriented dialogue', 'multitacred: a multilingual version of the tac relation extraction dataset', 'comparative evaluation of boundary-relaxed annotation for entity linking performance', 'do androids laugh at electric sheep? humor \"understanding\" benchmarks from the new yorker caption contest', 'backpack language models', 'empowering cross-lingual behavioral testing of nlp models with typological features', 'large language models are reasoning teachers', 'my side, your side and the evidence: discovering aligned actor groups and the narratives they weave', 'faithful question answering with monte-carlo planning', 'unnatural instructions: tuning language models with (almost) no human labor', 'instruction induction: from few examples to natural language task descriptions', 'attributable and scalable opinion summarization', 'misgendered: limits of large language models in understanding pronouns', 'resolving indirect referring expressions for entity selection', 'organ: observation-guided radiology report generation via tree reasoning', 'cone: an efficient coarse-to-fine alignment framework for long video temporal grounding', 'tagprime: a unified framework for relational structure extraction', 'ampere: amr-aware prefix for generation-based event argument extraction model', 'infometic: an informative metric for reference-free image caption evaluation', 'supervised adversarial contrastive learning for emotion recognition in conversations', 'a fine-grained comparison of pragmatic language understanding in humans and language models', 'won’t get fooled again: answering questions with false premises', 'in-context analogical reasoning with pre-trained language models', 'meetingbank: a benchmark dataset for meeting summarization', 'hearing lips in noise: universal viseme-phoneme mapping and transfer for robust audio-visual speech recognition', 'mir-gan: refining frame-level modality-invariant representations with adversarial network for audio-visual speech recognition', 'semantic structure enhanced event causality identification', 'improving translation quality estimation with bias mitigation', 'knowledge transfer in incremental learning for multilingual neural machine translation', 'paraamr: a large-scale syntactically diverse paraphrase dataset by amr back-translation', 'faking fake news for real fake news detection: propaganda-loaded training data generation', 'zero-shot faithful factual error correction', 'more than classification: a unified framework for event temporal relation extraction', 'av-transpeech: audio-visual robust speech-to-speech translation', 'an extensible plug-and-play method for multi-aspect controllable text generation', 'towards higher pareto frontier in multilingual machine translation', 'mvp-tuning: multi-view knowledge retrieval with prompt tuning for commonsense reasoning', 'red<sup>fm</sup>: a filtered and multilingual relation extraction dataset', 'glot500: scaling multilingual corpora and language models to 500 languages', 'unity: two-pass direct speech-to-speech translation with discrete units', 'byte-level grammatical error correction using synthetic and curated corpora', 'dare: towards robust text explanations in biomedical and healthcare applications', 'hint: hypernetwork instruction tuning for efficient zero- and few-shot generalisation', 'contraclm: contrastive learning for causal language model', 'knowledge unlearning for mitigating privacy risks in language models', 'seegull: a stereotype benchmark with broad geo-cultural coverage leveraging generative models', 'hierarchical verbalizer for few-shot hierarchical text classification', 'in-sample curriculum learning by sequence completion for natural language generation', 'modeling instance interactions for joint information extraction with neural high-order conditional random field', 'vision language pre-training by contrastive learning with cross-modal similarity regulation', 'recall, expand, and multi-candidate cross-encode: fast and accurate ultra-fine entity typing', 'llm-blender: ensembling large language models with pairwise ranking and generative fusion', 'a cognitive stimulation dialogue system with multi-source knowledge fusion for elders with cognitive impairment', 'pruning pre-trained language models without fine-tuning', 'discourse-centric evaluation of document-level machine translation with a new densely annotated parallel corpus of novels', 'improving domain generalization for prompt-aware essay scoring via disentangled representation learning', 'patton: language model pretraining on text-rich networks', 'darkbert: a language model for the dark side of the internet', 'multi-source semantic graph-based multimodal sarcasm explanation generation', 'ms-detr: natural language video localization with sampling moment-moment interaction', 'breeding machine translations: evolutionary approach to survive and thrive in the world of automated evaluation', 'u-creat: unsupervised case retrieval using events extraction', 'are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales', 'arganalysis35k : a large-scale dataset for argument quality analysis', 'a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports', 'node placement in argument maps: modeling unidirectional relations in high & low-resource scenarios', 'your spouse needs professional help: determining the contextual  appropriateness of messages through modeling social relationships', 'evaluating open-domain question answering in the era of large language models', 'distill or annotate? cost-efficient fine-tuning of compact models', 'lambada: backward chaining for automated reasoning in natural language', 'decompeval: evaluating generated texts as unsupervised decomposed question answering', 'few-shot reranking for multi-hop qa via language model prompting', 'a cautious generalization goes a long way: learning morphophonological rules', 'explainmeetsum: a dataset for explainable meeting summarization aligned with human intent', 'automatic creation of named entity recognition datasets by querying phrase representations', 'infoverse: a universal framework for dataset characterization with multidimensional meta-information', 'factkg: fact verification via reasoning on knowledge graphs', '(qa)^2: question answering with questionable assumptions', 'entity tracking in language models', 'clinical note owns its hierarchy: multi-level hypergraph neural networks for patient-level representation learning', 'micse: mutual information contrastive learning for low-shot sentence embeddings', 'pairspanbert: an enhanced language model for bridging resolution', 'morphological inflection: a reality check', 'memory-efficient nllb-200: language-specific expert pruning of a massively multilingual machine translation model', 'promptrank: unsupervised keyphrase extraction using prompt', 'improving the robustness of nli models with minimax training', 'ditto: data-efficient and fair targeted subset selection for asr accent adaptation', 'downstream datasets make surprisingly good pretraining corpora', 'multi-row, multi-span distant supervision for table+text question answering', 'an inclusive notion of text', 'language detoxification with attribute-discriminative latent space', 'vision meets definitions: unsupervised visual word sense disambiguation incorporating gloss information', 'swipe: a dataset for document-level simplification of wikipedia pages', 'drbert: a robust pre-trained model in french for biomedical and clinical domains', 'contrastive error attribution for finetuned language models', 'exploring better text image translation with multimodal codebook', 'what about \"em\"? how commercial machine translation fails to handle (neo-)pronouns', 'improved instruction ordering in recipe-grounded conversation', 'formnetv2: multimodal graph contrastive learning for form document information extraction', 'query-efficient black-box red teaming via bayesian optimization', 'on complementarity objectives for hybrid retrieval', 'square: a large-scale dataset of sensitive questions and acceptable responses created through human-machine collaboration', 'revealing single frame bias for video-and-language learning', 'tart: improved few-shot text classification using task-adaptive reference transformation', 'bic: twitter bot detection with text-graph interaction and semantic consistency', 'tell2design: a dataset for language-guided floor plan generation', 'diverse demonstrations improve in-context compositional generalization', 'few-shot data-to-text generation via unified representation and multi-source learning', 'understanding client reactions in online mental health counseling', 'toward interactive dictation', 'python code generation by asking clarification questions', 'do you hear the people sing? key point analysis via iterative clustering and abstractive summarisation', 'teast: temporal knowledge graph embedding via archimedean spiral timeline', 'can language models make fun? a case study in chinese comical crosstalk', 'contextual distortion reveals constituency: masked language models are implicit parsers', 'are message passing neural networks really helpful for knowledge graph completion?', 'cats: a pragmatic chinese answer-to-sequence dataset with large scale and high quality', 'text adversarial purification as defense against adversarial attacks', 'symbolic chain-of-thought distillation: small models can also \"think\" step-by-step', 'multi-modal action chain abductive reasoning', 'citadel: conditional token interaction via dynamic lexical routing for efficient and effective multi-vector retrieval', 'codeie: large code generation models are better few-shot information extractors', 'to copy rather than memorize: a vertical learning paradigm for knowledge graph completion', 'open-domain hierarchical event schema induction by incremental prompting and verification', 'sequence parallelism: long sequence training from system perspective', 'few-shot in-context learning on knowledge base question answering', 'trea: tree-structure reasoning schema for conversational recommendation', 'contrastive decoding: open-ended text generation as optimization', 'unified demonstration retriever for in-context learning', 'explicit syntactic guidance for neural text generation', 'attgen: attribute tree generation for real-world attribute joint extraction', 'multi-target backdoor attacks for code pre-trained models', 'translation-enhanced multilingual text-to-image generation', 'making language models better reasoners with step-aware verifier', 'templategec: improving grammatical error correction with detection template', 'prompt tuning pushes farther, contrastive learning pulls closer: a two-stage approach to mitigate social biases', 'a new direction in stance detection: target-stance extraction in the wild', 'pre-training multi-party dialogue models with latent discourse inference', 'em pre-training for multi-party dialogue response generation', 'multiview identifiers enhanced generative retrieval', 'dionysus: a pre-trained model for low-resource dialogue summarization', 'white-box multi-objective adversarial attack on dialogue generation', 'pace: unified multi-modal dialogue pre-training with progressive and compositional experts', 'a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text', 'a multi-modal context reasoning approach for conditional inference on joint textual and visual clues', 'unifying cross-lingual and cross-modal modeling towards weakly supervised multilingual vision-language pre-training', 'learning to substitute spans towards improving compositional generalization', 'faa: fine-grained attention alignment for cascade document ranking', 'the best of both worlds: combining human and machine translations for multilingual semantic parsing with active learning', 'dual-alignment pre-training for cross-lingual sentence embedding', 'dynamic and efficient inference for text generation via bert family', 'open-ended long text generation via masked language modeling', 'summary-oriented vision modeling for multimodal abstractive summarization', 'disentangled phonetic representation for chinese spelling correction', 'graph-based relation mining for context-free out-of-vocabulary word embedding learning', 'prompts can play lottery tickets well: achieving lifelong information extraction via lottery prompt tuning', 'parameter-efficient fine-tuning without introducing new latency', 'large-scale correlation analysis of automated metrics for topic models', 'gloss-free end-to-end sign language translation', 'techs: temporal logical graph networks for explainable extrapolation reasoning', 'tavt: towards transferable audio-visual text generation', 'an inner table retriever for robust table question answering', 'compositional generalization without trees using multiset tagging and latent permutations', 'what does a text classifier learn about morality? an explainable method for cross-domain comparison of moral rhetoric', 'dimongen: diversified generative commonsense reasoning for explaining concept relationships', 'matcha: enhancing visual language pretraining with math reasoning and chart derendering', 'learning with partial annotations for event detection', 'document-level event argument extraction with a chain reasoning paradigm', 'rankcse: unsupervised sentence representations learning via learning to rank', 'towards robust low-resource fine-tuning with multi-view compressed representations', 'do question answering modeling improvements hold across benchmarks?', 'character-aware models improve visual text rendering', 'recap: retrieval-enhanced context-aware prefix encoder for personalized dialogue response generation', 'knn-tl: k-nearest-neighbor transfer learning for low-resource neural machine translation', 'do conll-2003 named entity taggers still work well in 2023?', 'annotation-inspired implicit discourse relation classification with auxiliary discourse connective generation', 'mgr: multi-generator based rationalization', 'modeling structural similarities between documents for coherence assessment with graph convolutional networks', 'revisiting commonsense reasoning in machine translation: training, evaluation and challenge', 'one cannot stand for everyone! leveraging multiple user simulators\\\\  to train task-oriented dialogue systems', 'uncovering and categorizing social biases in text-to-sql', 'towards better entity linking with multi-view enhanced distillation', 'a crosslingual investigation of conceptualization in 1335 languages', 'revisiting the gold standard: grounding summarization evaluation with robust human evaluation', 'on improving summarization factual consistency from natural language feedback', 'pvgru: generating diverse and relevant dialogue responses via pseudo-variational mechanism', 'binary and ternary natural language generation', 'xdailydialog: a multilingual parallel dialogue corpus', 'retromae-2: duplex masked auto-encoder for pre-training retrieval-oriented language models', 'guiding computational stance detection with expanded stance triangle framework', 'towards boosting the open-domain chatbot with human feedback', 'what makes pre-trained language models better zero-shot learners?', 'facilitating fine-grained detection of chinese toxic language: hierarchical taxonomy, resources, and benchmarks', 'damstf: domain adversarial learning enhanced meta self-training for domain adaptation', 'distantly supervised course concept extraction in moocs with academic discipline', 'a survey of deep learning for mathematical reasoning', 'toward human-like evaluation for natural language generation with error analysis', 'explanation-based finetuning makes models more robust to spurious cues', 'causality-guided multi-memory interaction network for multivariate stock price movement prediction', 'hahe: hierarchical attention for hyper-relational knowledge graphs in global and local level', 'end-to-end knowledge retrieval with multi-modal queries', 'came: confidence-guided adaptive memory efficient optimization', 'dialogps: dialogue path sampling in continuous semantic space for data augmentation in multi-turn conversations', 'envisioning future from the past: hierarchical duality learning for multi-turn dialogue generation', 'z-icl: zero-shot in-context learning with pseudo-demonstrations', 'cross-lingual continual learning', 'amr-based network for aspect-based sentiment analysis', 'chain-of-skills: a configurable model for open-domain question answering', 'bump: a benchmark of unfaithful minimal pairs for meta-evaluation of faithfulness metrics', 'dice: data-efficient clinical event extraction with generative models', 'learning \"o\" helps for learning more: handling the unlabeled entity problem for class-incremental ner', 'colada: a collaborative label denoising framework for cross-lingual named entity recognition', 'few-shot event detection: an empirical study and a unified view', 'world-to-words: grounded open vocabulary acquisition through fast mapping in vision-language models', 'training models to generate, recognize, and reframe unhelpful thoughts', 'lens: a learnable evaluation metric for text simplification', 'ethical considerations for machine translation of indigenous languages: giving a voice to the speakers', 'hypermixer: an mlp-based low cost alternative to transformers', 'small data, big impact: leveraging minimal data for effective machine translation', 'quest: a retrieval dataset of entity-seeking queries with implicit set operations', 'when not to trust language models: investigating effectiveness of parametric and non-parametric memories', 'benchmarking large language model capabilities for conditional generation', 'logic-driven indirect supervision: an application to crisis counseling', 'resolving ambiguities in text-to-image generative models', 'notable: transferable backdoor attacks against prompt-based nlp models', 'on the efficacy of sampling adapters', 'from dogwhistles to bullhorns: unveiling coded rhetoric with language models', 'human-in-the-loop evaluation for early misinformation detection: a case study of covid-19 treatments', 'naamapadam: a large-scale named entity annotated data for indic languages', 'what do nlp researchers believe? results of the nlp community metasurvey', 'lait: efficient multi-segment encoding in transformers with layer-adjustable interaction', 'just like a human would, direct access to sarcasm augmented with potential result and reaction', 'where’s the point? self-supervised multilingual punctuation-agnostic sentence segmentation', 'privacy-preserving domain adaptation of semantic parsers', 'what is the real intention behind this question? dataset collection and intention classification', 'pal to lend a helping hand: towards building an emotion adaptive polite and empathetic counseling conversational agent', 'convgqr: generative query reformulation for conversational search', 'decompx: explaining transformers decisions by propagating token decomposition', 'extrinsic evaluation of machine translation metrics', 'large-scale lifelong learning of in-context instructions and how to tackle it', 'dynamic regularization in uda for transformers in multimodal classification', 'randomized smoothing with masked inference for adversarially robust text classifications', 'uppam: a unified pre-training architecture for political actor modeling based on language', 'decoupling pseudo label disambiguation and representation learning for generalized intent discovery', 'how to plant trees in language models: data and architectural effects on the emergence of syntactic inductive biases', 'crosslingual generalization through multitask finetuning', 'dip: dead code insertion based black-box attack for programming language model', 'efficient transformers with dynamic token pooling', 'disentqa: disentangling parametric and contextual knowledge with counterfactual question answering', 'when does aggregating multiple skills with multi-task learning work? a case study in financial nlp', 'finding the pillars of strength for multi-head attention', 'nlg evaluation metrics beyond correlation analysis: an empirical metric preference checklist', 'od-rte: a one-stage object detection framework for relational triple extraction', 'on \"scientific debt\" in nlp: a case for more rigour in language model pre-training research', 'using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning', 'token-wise decomposition of autoregressive language model hidden states for analyzing model predictions', 'social-group-agnostic bias mitigation via the stereotype content model', 'can lms learn new entities from descriptions? challenges in propagating injected knowledge', 'efficient semiring-weighted earley parsing', 'blind: bias removal with no demographics', 'a textual dataset for situated proactive response selection', 'songs across borders: singable and controllable neural lyric translation', 'waco: word-aligned contrastive learning for speech translation', 'compositional data augmentation for abstractive conversation summarization', 'on prefix-tuning for lightweight out-of-distribution detection', 'thinksum: probabilistic reasoning over sets using large language models', 'socratic pretraining: question-driven pretraining for controllable summarization', 'multitabqa: generating tabular answers for multi-table question answering', 'using neural machine translation for generating diverse challenging exercises for language learner', 'fact-checking complex claims with program-guided reasoning', 'cross-modal attention congruence regularization for vision-language relation alignment', 'reward gaming in conditional text generation', 'attention as a guide for simultaneous speech translation', 'mm-shap: a performance-agnostic metric for measuring multimodal contributions in vision and language models & tasks', 'geneva: benchmarking generalizability for event argument extraction with hundreds of event types and argument roles', 'deep model compression also helps models capture ambiguity', 'do i have the knowledge to answer? investigating answerability of knowledge base questions', 'beyond english-centric bitexts for better multilingual language representation learning', 'dating greek papyri with text regression', 'when to use what: an in-depth comparative empirical analysis of openie systems for downstream applications', 'fsuie: a novel fuzzy span mechanism for universal information extraction', 'are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark', 'neural machine translation for mathematical formulae', 'dealing with semantic underspecification in multimodal nlp', 'towards a common understanding of contributing factors for cross-lingual transfer in multilingual language models: a review', 'uniex: an effective and efficient framework for unified information extraction via a span-extractive perspective', 'learning language-specific layers for multilingual machine translation', 'multilingual multifaceted understanding of online news in terms of genre, framing, and persuasion techniques', 'no clues good clues: out of context lexical relation classification', 'similarity-weighted construction of contextualized commonsense knowledge graphs for knowledge-intense argumentation tasks', 'concise answers to complex questions: summarization of long-form answers', 'reanalyzing l2 preposition learning with bayesian mixed effects and a pretrained language model', 'meetingqa: extractive question-answering on meeting transcripts', 'using domain knowledge to guide dialog structure induction via neural probabilistic soft logic', 'conjunct lengths in english, dependency length minimization, and dependency structure of coordination', 'incorporating distributions of discourse structure for long document abstractive summarization', 'clarifydelphi: reinforced clarification questions with defeasibility rewards for social and moral situations', 'limitations of language models in arithmetic and symbolic induction', 'unilg: a unified structure-aware framework for lyrics generation', 'parals: lexical substitution via pretrained paraphraser', 'reasoning with language model prompting: a survey', 'learning to initialize: can meta learning improve cross-task generalization in prompt tuning?', 'webcpm: interactive web search for chinese long-form question answering', 'a survey on asking clarification questions datasets in conversational systems', 'what are you token about? dense retrieval as distributions over the vocabulary', 'single sequence prediction over reasoning graphs for multi-hop qa', 'a comparative study on the impact of model compression techniques on fairness in language models', 'knowledge of cultural moral norms in large language models', 'cross-modal attribute insertions for assessing the robustness of vision-and-language learning', 'factify-5wqa: 5w aspect-based fact verification through question answering', 'conjunct resolution in the face of verbal omissions', 'parallel context windows for large language models', 'linear guardedness and its implications', 'tome: a two-stage approach for model-based retrieval', 'retrieve-and-sample: document-level event argument extraction via hybrid retrieval augmentation', 'tailoring instructions to student’s learning levels boosts knowledge distillation', 'exploring large language models for classical philology', 'factually consistent summarization via reinforcement learning with textual entailment feedback', 'finding the sweet spot: analysis and improvement of adaptive inference in low resource settings', 'distributed marker representation for ambiguous discourse markers and entangled relations', 'a dataset of argumentative dialogues on scientific papers', 'helping a friend or supporting a cause? disentangling active and passive cosponsorship  in the u.s. congress', 'revisiting non-english text simplification: a unified multilingual benchmark', 'argu: a controllable factual argument generator', 'indicmt eval: a dataset to meta-evaluate machine translation metrics for indian languages', 'hidden schema networks', 'accelerating transformer inference for translation via parallel decoding', 'nlpositionality: characterizing design biases of datasets and models', 'apollo: a simple approach for adaptive pretraining of language models for logical reasoning', 'vendorlink: an nlp approach for identifying & linking vendor migrants & potential aliases on darknet markets', 'multilingual conceptual coverage in text-to-image models', 'tree-based representation and generation of natural and mathematical language', 'free lunch: robust cross-lingual transfer via model checkpoint averaging', 'minding language models’ (lack of) theory of mind: a plug-and-play multi-character belief tracker', 'ranking-enhanced unsupervised sentence representation learning', 'training-free neural architecture search for rnns and transformers', 'trillion dollar words: a new financial dataset, task & market analysis', 'causes and cures for interference in multilingual translation', 'on second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning', 'cognitive reframing of negative thoughts through human-language model interaction', 'learning non-linguistic skills without sacrificing linguistic proficiency', 'when and how to paraphrase for named entity recognition?', 'memex: detecting explanatory evidence for memes via knowledge-enriched contextualization', 'dense-atomic: towards densely-connected atomic with high knowledge coverage and massive multi-hop paths', 'promptner: prompt locating and typing for named entity recognition', 'diffusionner: boundary diffusion for named entity recognition', 'multiemo: an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations', 'midmed: towards mixed-type dialogues for medical consultation', 'rade: reference-assisted dialogue evaluation for open-domain dialogue', 'pivotal role of language modeling in recommender systems: enriching task-specific and task-agnostic representation learning', 'slue phase-2:  a benchmark suite of diverse spoken language understanding tasks', 'evaluate amr graph similarity via self-supervised learning', 'measuring inductive biases of in-context learning with underspecified demonstrations', 'readin: a chinese multi-task benchmark with realistic and diverse input noises', 'combo of thinking and observing for outside-knowledge vqa', 'learning to generate equitable text in dialogue from biased training data', 'big-c: a multimodal multi-purpose dataset for bemba', 'peeking inside the black box: a commonsense-aware generative framework for explainable complaint detection', 'forgotten knowledge: examining the citational amnesia in nlp', 'eel: efficiently encoding lattices for reranking', 'language model acceptability judgements are not always robust to context', 'fermat: an alternative to accuracy for numerical reasoning', 'to revise or not to revise: learning to detect improvable claims for argumentative writing support', 'a simple and flexible modeling for mental disorder detection by learning from clinical questionnaires', 'peer-label assisted hierarchical text classification', 'matsci-nlp: evaluating scientific language models on materials science language tasks using text-to-schema modeling', 'grounding characters and places in narrative text', 'unsupervised extractive summarization of emotion triggers', 'local byte fusion for neural machine translation', 'why did the chicken cross the road? rephrasing and analyzing ambiguous questions in vqa', 'deplain: a german parallel corpus with intralingual translations into plain language for sentence and document simplification', 'an ordinal latent variable model of conflict intensity', 'a causal framework to quantify the robustness of mathematical reasoning with language models', 'unsupervised selective rationalization with noise injection', 'nlp reproducibility for all: understanding experiences of beginners', 'wikibio: a semantic resource for the intersectional analysis of biographical events', 'history semantic graph enhanced conversational kbqa with temporal information modeling', 'moraldial: a framework to train and evaluate moral dialogue systems via moral discussions', 'dialect-robust evaluation of generated text', 'layer-wise fusion with modality independence modeling for multi-modal emotion recognition', 'from characters to words: hierarchical pre-trained language model for open-vocabulary language understanding', 'uncertainty guided label denoising for document-level distant relation extraction', 'multitask pre-training of modular prompt for chinese few-shot learning', 'backdooring neural code search', 'answering ambiguous questions via iterative prompting', 'a length-extrapolatable transformer', 'idrisi-ra: the first arabic location mention recognition dataset of disaster tweets', 'why aren’t we ner yet? artifacts of asr errors in named entity recognition in spontaneous speech transcripts', 'towards benchmarking and improving the temporal reasoning capability of large language models', 'vistext: a benchmark for semantically rich chart captioning', 'enhancing dialogue generation via dynamic graph knowledge aggregation', 'understanding factual errors in summarization: errors, summarizers, datasets, error detectors', 'what the daam: interpreting stable diffusion using cross attention', 'multilingual knowledge graph completion with language-sensitive multi-graph attention', 'learning to imagine: visually-augmented natural language generation', 'learning dynamic contextualised word embeddings via template-based temporal adaptation', 'enhancing personalized dialogue generation with contrastive latent variables: combining sparse and dense persona', 'hybrid transducer and attention based encoder-decoder modeling for speech-to-text tasks', 'multilingual llms are better cross-lingual in-context learners with alignment', 'core: cooperative training of retriever-reranker for effective dialogue response selection', 'unievent: unified generative model with multi-dimensional prefix for zero-shot event-relational reasoning', 'what’s the meaning of superhuman performance in today’s nlu?', 'we understand elliptical sentences, and language models should too: a new dataset for studying ellipsis and its interaction with thematic fit', 'being right for whose right reasons?', 'dynamic routing transformer network for multimodal sarcasm detection', 'unsupervised melody-to-lyrics generation', 'a new aligned simple german corpus', 'are fairy tales fair? analyzing gender bias in temporal narrative event chains of children’s fairy tales', 'model-based simulation for optimising smart reply', 'crest: a joint framework for rationalization and counterfactual text generation', 'interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions', 'layoutmask: enhance text-layout interaction in multi-modal pre-training for document understanding', 'is fine-tuning needed? pre-trained language models are near perfect for out-of-domain detection', 'curriculum learning for graph neural networks: a multiview competence-based approach', 'transfer and active learning for dissonance detection: addressing the rare-class challenge', 'post-abstention: towards reliably re-attempting the abstained instances in qa', 'hybrid uncertainty quantification for selective text classification in ambiguous tasks', 'prompting palm for translation: assessing strategies and performance', 'datafinder: scientific dataset recommendation from natural language descriptions', 'does gpt-3 grasp metaphors? identifying metaphor mappings with generative language models', 'revisiting relation extraction in the era of large language models', 'multi-grained knowledge retrieval for end-to-end task-oriented dialog', 'joint document-level event extraction via token-token bidirectional event completed graph', 'towards understanding chain-of-thought prompting: an empirical study of what matters', 'simple and effective unsupervised speech translation', 'aggregating multiple heuristic signals as supervision for unsupervised automated essay scoring', 'dt-solver: automated theorem proving with dynamic-tree sampling guided by proof-level value function', 'coad: automatic diagnosis through symptom and disease collaborative generation', 'towards unifying multi-lingual and cross-lingual summarization', 'easy guided decoding in providing suggestions for interactive machine translation', 'plan-and-solve prompting: improving zero-shot chain-of-thought reasoning by large language models', 'simlm: pre-training with representation bottleneck for dense passage retrieval', 'two-stage fine-tuning for improved bias and variance for large pretrained language models', 'a theory of unsupervised speech recognition', 'kga: a general machine unlearning framework based on knowledge gap alignment', 'a survey on zero pronoun translation', 'automated metrics for medical multi-document summarization disagree with human evaluations', 'scott: self-consistent chain-of-thought distillation', 'mustie: multimodal structural transformer for web information extraction', 'divide, conquer, and combine: mixture of semantic-independent experts for zero-shot dialogue state tracking', 'retrieval-free knowledge injection through multi-document traversal for dialogue models', 'recode: robustness evaluation of code generation models', 'better simultaneous translation with monotonic knowledge distillation', 'query structure modeling for inductive logical reasoning over knowledge graphs', 'cat: a contextualized conceptualization and instantiation framework for commonsense reasoning', 'elaboration-generating commonsense question answering at scale', 'effective contrastive weighting for dense query expansion', 'code4struct: code generation for few-shot event structure prediction', 'document-level multi-event extraction with event proxy nodes and hausdorff distance minimization', 'pesco: prompt-enhanced self contrastive learning for zero-shot text classification', 'weakly-supervised spoken video grounding via semantic interaction learning', 'element-aware summarization with large language models: expert-aligned evaluation and chain-of-thought method', 'self-instruct: aligning language models with self-generated instructions', 'dynamic heterogeneous-graph reasoning with language models and knowledge representation learning for commonsense question answering', 'greenkgc: a lightweight knowledge graph completion method', 'vstar: a video-grounded dialogue dataset for situated semantic understanding with scene and topic transitions', 'cola: contextualized commonsense causal reasoning from the causal inference perspective', 'rmlm: a flexible defense framework for proactively mitigating word-level adversarial attacks', 'rehearsal-free continual language learning via efficient parameter isolation', 'faithful low-resource data-to-text generation through cycle training', 'on evaluating multilingual compositional generalization with translated datasets', 'diffusiondb: a large-scale prompt gallery dataset for text-to-image generative models', 'what social attitudes about gender does bert encode? leveraging insights from psycholinguistics', 'subjective crowd disagreements for subjective data: uncovering meaningful crowdopinion with population-level learning', 'text style transfer back-translation', 'guide the many-to-one assignment: open information extraction via iou-aware optimal transport', 'tackling modality heterogeneity with multi-view calibration network for multimodal sentiment detection', 'f-divergence minimization for sequence-level knowledge distillation', 'webie: faithful and robust information extraction on the web', 'trigger warning assignment as a multi-label document classification problem', 'beyond contrastive learning: a variational generative model for multilingual retrieval', 'break: breaking the dialogue state tracking barrier with beam search and re-ranking', 'lilgym: natural language visual reasoning with reinforcement learning', 'rethinking masked language modeling for chinese spelling correction', 'connective prediction for implicit discourse relation recognition via knowledge distillation', 'multi-level knowledge distillation for out-of-distribution detection in text', 'wspalign: word alignment pre-training via large-scale weakly supervised span prediction', 'ambiguous learning from retrieval: towards zero-shot semantic parsing', 'denoising bottleneck with mutual information maximization for video multimodal fusion', 'information screening whilst exploiting! multimodal relation extraction with feature denoising and multimodal topic modeling', 'cross2stra: unpaired cross-lingual image captioning with cross-lingual cross-modal structure-pivoted alignment', 'ad-kd: attribution-driven knowledge distillation for language model compression', 'simmc-vr: a task-oriented multimodal dialog dataset with situated and immersive vr streams', 'learning action conditions from instructional manuals for instruction understanding', 'towards zero-shot multilingual transfer for code-switched responses', 'do plms know and understand ontological knowledge?', 'estimating the uncertainty in emotion attributes using deep evidential regression', 'wecheck: strong factual consistency checker via weakly supervised learning', 'self-adaptive in-context learning: an information compression perspective for in-context example selection and ordering', 'are experts needed? on human evaluation of counselling reflection generation', 'unicorn: unified cognitive signal reconstruction bridging cognitive signals and human language', 'training trajectories of language models across scales', 'plug-and-play document modules for pre-trained models', 'cfsum：a coarse-to-fine contribution network for multimodal summarization', 'an empirical analysis of parameter-efficient methods for debiasing pre-trained language models', 'interpreting positional information in perspective of word order', 'shrinking embeddings for hyper-relational knowledge graphs', 'contrastive novelty-augmented learning: anticipating outliers with large language models', 's2ynre: two-stage self-training with synthetic data for low-resource relation extraction', 'ctc-based non-autoregressive speech translation', 'introducing semantics into speech encoders', 'a critical evaluation of evaluations for long-form question answering', 'a universal discriminator for zero-shot generalization', 'double-branch multi-attention based graph neural network for knowledge graph completion', 'best-k search algorithm for neural text generation', 'can nli provide proper indirect supervision for low-resource biomedical relation extraction?', 'learning new skills after deployment: improving open-domain internet-driven dialogue with human feedback', 'towards open-world product attribute mining: a lightly-supervised approach', 'enhancing language representation with constructional information for natural language understanding', 'berm: training the balanced and extractable representation for matching to improve generalization ability of dense retrieval', 'peerda: data augmentation via modeling peer relation for span identification tasks', 'counterfactual debiasing for fact verification', 'sescore2: learning text generation evaluation via synthesizing realistic mistakes', 'managertower: aggregating the insights of uni-modal experts for vision-language representation learning', 'kilm: knowledge injection into encoder-decoder language models', 'exploring and verbalizing academic ideas by concept co-occurrence', 'unsupervised graph-text mutual conversion with a unified pretrained language model', 'hard sample aware prompt-tuning', 'multiinstruct: improving multi-modal zero-shot learning via instruction tuning', 'constrained tuple extraction with interaction-aware network', 'towards identifying fine-grained depression symptoms from memes', 'slabert talk pretty one day: modeling second language acquisition with bert', 'gec-depend: non-autoregressive grammatical error correction with decoupled permutation and decoding', 'holographic ccg parsing', 'holistic prediction on a time-evolving attributed graph', 'utc-ie: a unified token-pair classification architecture for information extraction', 'learning to simulate natural language feedback for interactive semantic parsing', 'bite: textual backdoor attacks with iterative trigger injection', 'bleurt has universal translations: an analysis of automatic metrics by minimum risk training', 'multicapclip: auto-encoding prompts for zero-shot multilingual visual captioning', 'efficient shapley values estimation by amortization for text classification', 'attractive storyteller: stylized visual storytelling with unpaired text', 'learning better masking for better language model pre-training', 'ganlm: encoder-decoder pre-training with an auxiliary discriminator', 'confede: contrastive feature decomposition for multimodal sentiment analysis', 'doc: improving long story coherence with detailed outline control', 'fantastic expressions and where to find them: chinese simile generation with multiple constraints', 'tailor: a soft-prompt-based approach to attribute-based controlled text generation', 'measuring consistency in text-based financial forecasting models', 'local interpretation of transformer based on linear decomposition', 'a new dataset and empirical study for sentence simplification in chinese', 'unsupervised discontinuous constituency parsing with mildly context-sensitive grammars', 'don’t parse, choose spans! continuous and discontinuous constituency parsing via autoregressive span selection', 'histred: a historical document-level relation extraction dataset', 'prototype-guided pseudo labeling for semi-supervised text classification', 'few-shot document-level event argument extraction', 'transforming visual scene graphs to image captions', 'an amr-based link prediction approach for document-level event argument extraction', 'gradient-based intra-attention pruning on pre-trained language models', 'are human explanations always helpful? towards objective evaluation of human natural language explanations', 'modeling user satisfaction dynamics in dialogue via hawkes process', 'multi-source test-time adaptation as dueling bandits for extractive question answering', 'fid-icl: a fusion-in-decoder approach for efficient in-context learning', 'clapspeech: learning prosody from text context with contrastive language-audio pre-training', 'how poor is the stimulus? evaluating hierarchical generalization in neural networks trained on child-directed speech', 'did you read the instructions? rethinking the effectiveness of task definitions in instruction learning', 'natural language to code generation in interactive data science notebooks', 'nuwa-xl: diffusion over diffusion for extremely long video generation', 'consistency regularization training for compositional generalization', 'bloom+1: adding language support to bloom for zero-shot prompting', 'rethinking annotation: can language learners contribute?', 'robust multi-bit natural language watermarking through invariant features', 'towards standardizing korean grammatical error correction: datasets and annotation', 'grounded multimodal named entity recognition on social media', 'cross-domain data augmentation with domain-adaptive language modeling for aspect-based sentiment analysis', 'word sense extension', 'personality understanding of fictional characters during book reading', 'alert: adapt language models to reasoning tasks', 'speech-text pre-training for spoken dialog understanding with explicit cross-modal alignment', 'generating hashtags for short-form videos with guided signals', 'crepe: open-domain question answering with false presuppositions', 'cold-start data selection for better few-shot language model fine-tuning: a prompt-based uncertainty propagation approach', 'augmentation-adapted retriever improves generalization of language models as generic plug-in', 'discriminative reasoning with sparse event representation for document-level event-event relation extraction', 'hype: better pre-trained language model fine-tuning with hidden representation perturbation', 'distilling script knowledge from large language models for constrained language planning', 'causality-aware concept extraction based on knowledge-guided prompting', 'synthetic text generation with differential privacy: a simple and practical recipe', 'metaadapt: domain adaptive few-shot misinformation detection via meta learning', 'zero- and few-shot event detection via prompt-based meta learning', 'movie101: a new movie understanding benchmark', 'large language models meet nl2code: a survey', 'one network, many masks: towards more parameter-efficient transfer learning', 'synthesize, prompt and transfer: zero-shot conversational question generation with pre-trained language model', 'soft language clustering for multilingual model pre-training', 'futuretod: teaching future knowledge to pre-trained language model for task-oriented dialogue', 'seen to unseen: exploring compositional generalization of multi-attribute controllable dialogue generation', 'cross-view language modeling: towards unified cross-lingual cross-modal pre-training', 'hints on the data for language modeling of synthetic languages with transformers', 'alignscore: evaluating factual consistency with a unified alignment function', 'ussa: a unified table filling scheme for structured sentiment analysis', 'contrastive learning with adversarial examples for alleviating pathology of language model', 'test-time adaptation for machine translation evaluation by uncertainty minimization', 'lifting the curse of capacity gap in distilling language models', 'dualgats: dual graph attention networks for emotion recognition in conversations', 'dual class knowledge propagation network for multi-label few-shot intent detection', 'understanding and improving the robustness of terminology constraints in neural machine translation', 'reasoning over hierarchical question decomposition tree for explainable question answering', 'what is overlap knowledge in event argument extraction? ape: a cross-datasets transfer learning model for eae', 'self-edit: fault-aware code editor for code generation', 'fc-kbqa: a fine-to-coarse composition framework for knowledge base question answering', 'a needle in a haystack: an analysis of high-agreement workers on mturk for summarization', 'span-level aspect-based sentiment analysis via table filling', 'learning latent relations for temporal knowledge graph reasoning', 'interpretable math word problem solution generation via step-by-step planning', 'safeconv: explaining and correcting conversational unsafe behavior', 'escoxlm-r: multilingual taxonomy-driven pre-training for the job market domain', 'a survey for efficient open domain question answering', 'a novel table-to-graph generation approach for document-level joint entity and relation extraction', 'mixce: training autoregressive language models by mixing forward and reverse cross-entropies', 'extractive is not faithful: an investigation of broad unfaithfulness problems in extractive summarization', 'towards understanding and improving knowledge distillation for neural machine translation', 'federated learning for semantic parsing: task formulation, evaluation setup, new algorithms', 'bridging the gap: entailment fused-t5 for open-retrieval conversational machine reading comprehension', 'a cross-modality context fusion and semantic refinement network for emotion recognition in conversation', 'mil-decoding: detoxifying language models at token-level via multiple instance learning', 'continual knowledge distillation for neural machine translation', 'vln-trans: translator for the vision and language navigation agent', 'xsemplr: cross-lingual semantic parsing in multiple natural languages and meaning representations', 'plug-and-play knowledge injection for pre-trained language models', 'dialog-post: multi-level self-supervised objectives and hierarchical model for dialogue post-training', 'ethicist: targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation', 'fine-tuning happens in tiny subspaces: exploring intrinsic task-specific subspaces of pre-trained language models', 'fedlegal: the first real-world federated learning benchmark for legal nlp', 'c-stance: a large dataset for chinese zero-shot stance detection', 'infusing hierarchical guidance into prompt tuning: a parameter-efficient framework for multi-level implicit discourse relation recognition', 'chbias: bias evaluation and mitigation of chinese conversational language models', 're-matching: a fine-grained semantic matching method for zero-shot relation extraction', 'open set relation extraction via unknown-aware training', 'actively supervised clustering for open relation extraction', 'evaluating open-domain dialogues in latent space with next sentence prediction and mutual information', 'verify-and-edit: a knowledge-enhanced chain-of-thought framework', 'abductive commonsense reasoning exploiting mutually exclusive explanations', 'improving continual relation extraction by distinguishing analogous semantics', 'pre-trained language models can be fully zero-shot learners', 'robut: a systematic study of table qa robustness against human-annotated adversarial perturbations', 'generating visual spatial description via holistic 3d scene understanding', 'incorporating attribution importance for improving faithfulness metrics', 'knowledgeable parameter efficient tuning network for commonsense question answering', 'an invariant learning characterization of controlled text generation', 'rethinking multimodal entity and relation extraction from a translation point of view', 'preserving commonsense knowledge from pre-trained language models via causal inference', 'generating structured pseudo labels for noise-resistant zero-shot video sentence localization', 'im-tqa: a chinese table question answering dataset with implicit and multi-type table structures', 'contextual knowledge learning for dialogue generation', 'a facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations', 'robust representation learning with reliable pseudo-labels generation via self-adaptive optimal transport for short text clustering', 'jointprop: joint semi-supervised learning for entity and relation extraction with heterogeneous graph-based propagation', 'natlogattack: a framework for attacking natural language inference models with natural logic', 'revisiting token dropping strategy in efficient bert pretraining', 'causal-debias: unifying debiasing in pretrained language models and fine-tuning via causal invariant learning', 'clcl: non-compositional expression detection with contrastive learning and curriculum learning', 'case: aligning coarse-to-fine cognition and affection for empathetic response generation', 'facilitating multi-turn emotional support conversation with positive emotion elicitation: a reinforcement learning approach', 'simoap: improve coherence and consistency in persona-based dialogue generation via over-sampling and post-evaluation', 'i cast detect thoughts: learning to converse and guide with intents and theory-of-mind in dungeons and dragons', 'bridging the gap between decision and logits in decision-based knowledge distillation for pre-trained language models', 'improving self-training for cross-lingual named entity recognition with contrastive and prototype learning', 'continual contrastive finetuning improves low-resource relation extraction', 'cmot: cross-modal mixup via optimal transport for speech translation', 'flame: few-shot learning from natural language explanations', 'non-sequential graph script induction via multimedia grounding', 'two birds one stone: dynamic ensemble for ood intent classification', 'a probabilistic framework for discovering new intents', 'fireball: a dataset of dungeons and dragons actual-play with structured game state information', 'weaker than you think: a critical look at weakly supervised learning', 'neural machine translation methods for translating text to sign language glosses', 'hitin: hierarchy-aware tree isomorphism network for hierarchical text classification', 'paed: zero-shot persona attribute extraction in dialogues', 'annotating and detecting fine-grained factual errors for dialogue summarization', 'peit: bridging the modality gap with pre-trained models for end-to-end image translation', 'ink: injecting knn knowledge in nearest neighbor machine translation', 'solving math word problems via cooperative reasoning induced language models', 'storytrans: non-parallel story author-style transfer with discourse representations and content enhancing', 'pretrained bidirectional distillation for machine translation', 'whitenedcse: whitening-based contrastive learning of sentence embeddings', 'modeling appropriate language in argumentation', 'normbank: a knowledge bank of situational social norms', 'multi-value: a framework for cross-dialectal english nlp', 'towards understanding omission in dialogue summarization', 'tokenization and the noiseless channel', 'soft alignment objectives for robust adaptation of language generation', 'lm-cppf: paraphrasing-guided data augmentation for contrastive prompt-based few-shot fine-tuning', 'the mechanical bard: an interpretable machine learning approach to shakespearean sonnet generation', 'learning neuro-symbolic world models with conversational proprioception', 'tecs: a dataset and benchmark for tense consistency of machine translation', 'is anisotropy truly harmful? a case study on text clustering', 'the role of global and local context in named entity recognition', 'hexatagging: projective dependency parsing as tagging', 'nichelle and nancy: the influence of demographic attributes and tokenization length on first name biases', 'split-ner: named entity recognition via two question-answering-based classifications', 'faithfulness tests for natural language explanations', 'a simple and effective framework for strict zero-shot hierarchical classification', 'decomposed scoring of ccg dependencies', 'efficient diagnosis assignment using unstructured clinical notes', 'an open dataset and model for language identification', 'evaluating zero-shot event structures: recommendations for automatic content extraction (ace) annotations', 'graph propagation based data augmentation for named entity recognition', 'substitution-based semantic change detection using contextual embeddings', 'xl-lexeme: wic pretrained model for cross-lingual lexical semantic change', 'controllable mixed-initiative dialogue generation through prompting', 'xsim++: an improved proxy to bitext mining performance for low-resource languages', 'toward expanding the scope of radiology report summarization to multiple anatomies and modalities', 'text-to-sql error correction with language models of code', 'task-aware specialization for efficient and robust dense retrieval for open-domain question answering', 'plue: language understanding evaluation benchmark for privacy policies in english', 'latent positional information is in the self-attention variance of transformer language models without positional embeddings', 'should you marginalize over possible tokenizations?', 'leveraging prefix transfer for multi-intent text revision', 'black-box language model explanation by context length probing', 'scaling in cognitive modelling: a multilingual approach to human reading times', 'zero-shot cross-lingual transfer with learned projections using unlabeled target-language data', 'context-aware transformer pre-training for answer sentence selection', 'when to use efficient self attention? profiling text, speech and image transformer variants', 'improving factuality of abstractive summarization without sacrificing summary quality', 'surface-based retrieval reduces perplexity of retrieval-augmented language models', 'improving gender fairness of pre-trained language models without catastrophic forgetting', 'reasoning implicit sentiment with chain-of-thought prompting', 'using contradictions improves question answering systems', 'mind the gap between the application track and the real world', 'analyzing text representations by measuring task alignment', 'morphological inflection with phonological features', 'detoxifying text with marco: controllable revision with experts and anti-experts', 'modality adaption or regularization? a case study on end-to-end speech translation', 'ellipsis-dependent reasoning: a new challenge for large language models', 'characterization of stigmatizing language in medical records', 'buca: a binary classification approach to unsupervised commonsense question answering', 'chatgpt for zero-shot dialogue state tracking: a solution or an opportunity?', 'contrastive bootstrapping for label refinement', 'multimodal relation extraction with cross-modal retrieval and synthesis', 'multitool-cot: gpt-3 can use multiple external tools with chain of thought prompting', 'know how to make up your mind! adversarially detecting and alleviating inconsistencies in natural language explanations', 'an (unhelpful) guide to selecting the best asr architecture for your under-resourced language', 'discourse-level representations can improve prediction of degree of anxiety', 'bring more attention to syntactic symmetry for automatic postediting of high-quality machine translations', 'table and image generation for investigating knowledge of entities in pre-trained vision and language models', 'stop pre-training: adapt visual-language models to unseen languages', 'a better way to do masked language model scoring', 'tracing linguistic markers of influence in a large online organisation', 'transformed protoform reconstruction', 'probing physical reasoning with counter-commonsense context', 'do models really learn to follow instructions? an empirical study of instruction tuning', 's3hqa: a three-stage approach for multi-hop text-table hybrid question answering', 'narrowbert: accelerating masked language model pretraining and inference', 'hipool: modeling long documents using graph neural networks', 'how well apply simple mlp to incomplete utterance rewriting?', 'counterfactual reasoning: testing language models’ understanding of hypothetical scenarios', 'prefix propagation: parameter-efficient tuning for long sequences', 'diversity-aware coherence loss for improving neural topic models', 'autoconv: automatically generating information-seeking conversations with large language models', 'metaphor detection via explicit basic meanings modelling', 'uncertainty-aware bootstrap learning for joint extraction on distantly-supervised data', 'li-rage: late interaction retrieval augmented generation with explicit signals for open-domain table question answering', 'linear classifier: an often-forgotten baseline for text classification', 'are sample-efficient nlp models more robust?', 'bolt: fast energy-based controlled text generation with tunable biases', 'molxpt: wrapping molecules with text for generative pre-training', 'twistlist: resources and baselines for tongue twister generation', 'improving grammar-based sequence-to-sequence modeling with decomposition and constraints', 'event extraction as question generation and answering', 'a study on the efficiency and generalization of light hybrid retrievers', 'parameter-efficient weight ensembling facilitates task-level knowledge transfer', 'focused prefix tuning for controllable text generation', 'improving syntactic probing correctness and robustness with control tasks', 'bhasa-abhijnaanam: native-script and romanized language identification for 22 indic languages', 'dataset distillation with attention labels for fine-tuning bert', 'teaching small language models to reason', 'unitrec: a unified text-to-text transformer and joint contrastive learning framework for text-based recommendation', 'exploring the impact of layer normalization for zero-shot neural machine translation', 'amrs assemble! learning to ensemble with autoregressive models for amr parsing', 'theory-grounded computational text analysis', 'a natural bias for language generation models', 'trading syntax trees for wordpieces: target-oriented opinion words extraction with wordpieces and aspect enhancement', 'deep active learning for morphophonological processing', 'mokb6: a multilingual open knowledge base completion benchmark', 'metavl: transferring in-context learning ability from language models to vision-language models', 'enhancing event causality identification with counterfactual reasoning', 'grokking of hierarchical structure in vanilla transformers', 'considerations for meaningful sign language machine translation based on glosses', 'simple augmentations of logical rules for neuro-symbolic knowledge graph completion', 'class based influence functions for error detection', 'a fast algorithm for computing prefix probabilities', 'self-distilled quantization: achieving high compression rates in transformer-based language models', 'the ecological fallacy in annotation: modeling human label variation goes beyond sociodemographics', 'controlling the extraction of memorized data from large language models via prompt-tuning', 'token-level self-evolution training for sequence-to-sequence learning', 'credible without credit: domain experts assess generative language models', 'stt4sg-350: a speech corpus for all swiss german dialect regions', 'unsupervised subtitle segmentation with masked language models', 'multi-document summarization with centroid-based pretraining', 'covering uncommon ground: gap-focused question generation for answer assessment', 'improving generalization in language model-based text-to-sql semantic parsing: two simple semantic boundary-based techniques', 'do gpts produce less literal translations?', 'miread: simple method for learning high-quality representations from scientific documents', 'the inside story: towards better understanding of machine translation neural evaluation metrics', 'randomized positional encodings boost length generalization of transformers', 'ramp: retrieval and attribute-marking enhanced prompting for attribute-controlled translation', 'actc: active threshold calibration for cold-start knowledge graph completion', 'the tail wagging the dog: dataset construction biases of social bias benchmarks', 'summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success)', 'class-incremental learning based on label generation', 'scone: benchmarking negation reasoning in language models with fine-tuning and in-context learning', 'nollysenti: leveraging transfer learning and machine translation for nigerian movie sentiment classification', 'detecting contradictory covid-19 drug efficacy claims from biomedical literature', 'joint end-to-end semantic proto-role labeling', 'environmental claim detection', 'with a little push, nli models can robustly and efficiently predict faithfulness', 'modular visual question answering via code generation', 'balancing lexical and semantic quality in abstractive summarization', 'towards fewer hallucinations in knowledge-grounded dialogue generation via augmentative and contrastive knowledge-dialogue', 'measuring the effect of influential messages on varying personas', 'are pre-trained language models useful for model ensemble in chinese grammatical error correction?', 'bootstrapping neural relation and explanation classifiers', 'typo-robust representation learning for dense retrieval', 'language models get a gender makeover: mitigating gender bias with few-shot data interventions', 'deriving language models from masked language models', 'evaluating pragmatic abilities of image captioners on a3ds', 'on the interpretability and significance of bias metrics in texts: a pmi-based approach', 'abstractive summarizers are excellent extractive summarizers', 'evaluating paraphrastic robustness in textual entailment models', 'improving automatic quotation attribution in literary novels', 'going beyond sentence embeddings: a token-level matching algorithm for calculating semantic textual similarity', 'mospc: mos prediction based on pairwise comparison', 'the art of prompting: event detection based on type specific prompts', 'let me check the examples: enhancing demonstration learning via explicit imitation', 'learning multi-step reasoning by solving arithmetic tasks', 'how to distill your bert: an empirical study on the impact of weight initialisation and distillation objectives', 'zero-shot and few-shot stance detection on varied topics via conditional generation', 'a holistic approach to reference-free evaluation of machine translation', 'listener model for the photobook referential game with clipscores as implicit reference chain', 'debiasing generative named entity recognition by calibrating sequence likelihood', 'mpmr: a multilingual pre-trained machine reader at scale', 'exploring continual learning for code generation models', 'an embarrassingly easy but strong baseline for nested named entity recognition', 'in and out-of-domain text adversarial robustness via label smoothing', 'a weakly supervised classifier and dataset of white supremacist language', 'gradient ascent post-training enhances language model generalization', 'back to patterns: efficient japanese morphological analysis with feature-sequence trie', 'target-based offensive language identification', 'cogen: abductive commonsense language generation', 'reaugkd: retrieval-augmented knowledge distillation for pre-trained language models', 'a simple concatenation can effectively improve speech translation', 'understanding demonstration-based learning from a causal perspective', 'towards adaptive prefix tuning for parameter-efficient language model fine-tuning', 'revisiting automated prompting: are we actually doing better?', 'robust learning for multi-party addressee recognition with discrete addressee codebook']\n"
     ]
    }
   ],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Prepare the text data\n",
    "text_data = [\" \".join(word_tokenize(title.lower())) for title in titles]\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC MODELING  Latent Dirichlet allocation, LDA (문서 전체의 단어를 분석할 때에 더 효율적일 것으로 보임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kimsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kimsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "generation text learning dialogue dataset model knowledge via multilingual question\n",
      "Topic 2:\n",
      "extraction knowledge via learning relation event translation speech text distillation\n",
      "Topic 3:\n",
      "learning generation translation machine extraction representation language evaluation text detection\n",
      "Topic 4:\n",
      "language models large learning understanding recognition towards entity reasoning model\n",
      "Topic 5:\n",
      "learning generalization answering question compositional using via embeddings without code\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Prepare the text data by tokenizing and removing stopwords\n",
    "def preprocess_titles(titles):\n",
    "    processed_titles = []\n",
    "    for title in titles:\n",
    "        words = word_tokenize(title.lower())\n",
    "        filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        processed_titles.append(\" \".join(filtered_words))\n",
    "    return processed_titles\n",
    "\n",
    "text_data = preprocess_titles(titles)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42) # n_components개의 토픽 # max_iter=max_iter 만큼 최대 반복\n",
    "lda.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10 #각 토픽에서 상위 OO 개의 단어\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdisplay_topics\u001b[49m(lda, feature_names, no_top_words)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'display_topics' is not defined"
     ]
    }
   ],
   "source": [
    "display_topics(lda, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토픽 1:\n",
    "Generation of Text and Dialogue Using Multilingual Knowledge\n",
    "\n",
    "이 토픽은 다국적 지식을 활용한 텍스트 및 대화 생성에 관한 것입니다. 주요 단어들인 \"generation\", \"text\", \"learning\", \"dialogue\", \"dataset\", \"model\", \"knowledge\", \"via\", \"multilingual\", \"question\" 등은 다국적 데이터와 지식을 기반으로 한 텍스트 생성 및 대화 모델링에 중점을 둔 연구들을 나타낼 수 있습니다.\n",
    "\n",
    "토픽 2:\n",
    "Knowledge Extraction and Relation Learning Across Texts\n",
    "\n",
    "이 토픽은 텍스트 간의 관계 학습과 지식 추출에 관한 것입니다. 주요 단어들인 \"extraction\", \"knowledge\", \"learning\", \"relation\", \"event\", \"translation\", \"speech\", \"text\", \"distillation\" 등은 텍스트에서 지식을 추출하고 관계를 학습하는 연구에 중점을 둔 주제를 나타냅니다.\n",
    "\n",
    "토픽 3:\n",
    "Language Generation and Translation Evaluation\n",
    "\n",
    "이 토픽은 언어 생성, 번역 평가에 관한 것입니다. 주요 단어들인 \"learning\", \"generation\", \"translation\", \"machine\", \"extraction\", \"representation\", \"language\", \"evaluation\", \"text\", \"detection\" 등은 언어 생성과 번역 평가에 관련된 연구를 나타냅니다.\n",
    "\n",
    "토픽 4:\n",
    "Large Language Models and Understanding Towards Entity Recognition\n",
    "\n",
    "이 토픽은 대형 언어 모델과 엔티티 인식을 향한 이해에 관한 것입니다. 주요 단어들인 \"language\", \"models\", \"large\", \"learning\", \"understanding\", \"recognition\", \"towards\", \"entity\", \"reasoning\", \"model\" 등은 대규모 언어 모델을 활용한 엔티티 인식 및 이해에 대한 연구를 의미합니다.\n",
    "\n",
    "토픽 5:\n",
    "Learning and Generalization in Question Answering Without Code\n",
    "\n",
    "이 토픽은 코드 없이 질문 답변에서의 학습과 일반화에 관한 것입니다. 주요 단어들인 \"learning\", \"generalization\", \"answering\", \"question\", \"compositional\", \"using\", \"via\", \"embeddings\", \"without\", \"code\" 등은 코드 없이 학습된 모델이 질문 답변 과제에서 어떻게 학습하고 일반화하는지에 대한 연구를 나타냅니다.                                                 \n",
    "\n",
    "\n",
    "   GPT-3 의 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1: multilingual , text , generation ? 다국어 문장 생성? \n",
    "generation text learning dialogue dataset model knowledge via multilingual question\n",
    "Topic 2: extraction  , relation , 추출 , 관계 distillation 증류? -> 비전공자는 뭔지 모름 아는놈은 알수도?\n",
    "extraction knowledge via learning relation event translation speech text distillation\n",
    "Topic 3: translation , extraction  , languange , evaluation , detecrion , 번역 , 추출 ,언어 , 평가  번역된 텍스트에 대한 평가? \n",
    "learning generation translation machine extraction representation language evaluation text detection\n",
    "Topic 4: large , recognition ,entity  ,대형모델 , 인식 ? \n",
    "language models large learning understanding recognition towards entity reasoning model\n",
    "Topic 5: without code answering question generalization  일반화 ,  문답 , 코드없이 ?\n",
    "learning generalization answering question compositional using via embeddings without code\n",
    "\n",
    "단어만 특정해서 보면 중복되는 단어가 많음 , 주제를 단번에 파악하기에는 어려우나, 그것이 비전공자의 한계인지는 잘 모름 넓고 얕은 지식으로는 이정도의 추론? \n",
    "\n",
    "GPT의 해석을 통해서 보면 각 토픽마다 핵심이되는 단어가 몇개 있는것 같음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kimsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kimsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m  \u001b[38;5;66;03m# 최대 반복 횟수 설정\u001b[39;00m\n\u001b[0;32m     32\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39mn_topics, max_iter\u001b[38;5;241m=\u001b[39mmax_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Display the top n-grams for each topic\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_topics\u001b[39m(model, feature_names, no_top_words):\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:672\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    665\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    666\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    667\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    668\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    669\u001b[0m         )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 466\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[0;32m    480\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\code\\small_talk\\2. trend_follower\\development\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:144\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    140\u001b[0m last_d \u001b[38;5;241m=\u001b[39m doc_topic_d\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m norm_phi \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_topic_word_d\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eps\n\u001b[0;32m    146\u001b[0m doc_topic_d \u001b[38;5;241m=\u001b[39m exp_doc_topic_d \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts \u001b[38;5;241m/\u001b[39m norm_phi, exp_topic_word_d\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Prepare the text data by tokenizing and removing stopwords\n",
    "def preprocess_titles(titles):\n",
    "    processed_titles = []\n",
    "    for title in titles:\n",
    "        words = word_tokenize(title.lower())\n",
    "        filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        processed_titles.append(\" \".join(filtered_words))\n",
    "    return processed_titles\n",
    "\n",
    "text_data = preprocess_titles(titles)\n",
    "\n",
    "# Vectorize the text data with n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))  # n-gram range 설정 -> 같이 쓰이는 단어\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Fit LDA model\n",
    "n_topics = 5  # 토픽 개수 설정\n",
    "max_iter = 5000  # 최대 반복 횟수 설정\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, random_state=42)\n",
    "lda.fit(X)\n",
    "# Display the top n-grams for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        ngrams = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1] if ' ' in feature_names[i]]  # n-gram만 필터링\n",
    "        print(\", \".join(ngrams))\n",
    "        print()  # 각 토픽 사이에 빈 줄 추가추가\n",
    "\n",
    "# 주제와 상위 단어 출력\n",
    "no_top_words = 10  # 각 토픽에서 출력할 상위 단어 개수 설정\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "language, models, language models, large, generation, large language, large language models, learning, extraction, text\n",
      "\n",
      "Topic 2:\n",
      "learning, detection, models, knowledge, via, language, domain, reasoning, translation, generation\n",
      "\n",
      "Topic 3:\n",
      "translation, language, generation, models, text, summarization, machine translation, machine, dataset, understanding\n",
      "\n",
      "Topic 4:\n",
      "language, learning, models, language models, via, extraction, graph, dataset, model, text\n",
      "\n",
      "Topic 5:\n",
      "language, learning, models, generation, language models, via, knowledge, evaluation, dialogue, model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
